# -*- coding: utf-8 -*-
"""isolation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16bAXdLVe6TXp8OsS7riBCPTNHLoQd61t
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

RANDOM_STATE = 42

df=pd.read_csv('industrial_sensor_data.csv')

df.head()

df.shape

df.columns

df.isnull().sum()

df.nunique()

duplicated_in_columns = {}
for column in df.columns:
    duplicated_in_columns[column] = df[column].duplicated().sum()
df_duplicated_in_columns = pd.Series(duplicated_in_columns)
print(df_duplicated_in_columns)

date_cols = ['LastServiceDate']  # adjust if different
for c in date_cols:
    df[c] = pd.to_datetime(df[c], dayfirst=True, errors='coerce')  # handle formats like DD-MM-YYYY

# convert sensor numeric-ish columns
numeric_cols = ['Temperature','Pressure','Vibration','Humidity','FlowRate','PowerConsumption','RuntimeHours','SensorID']
for c in numeric_cols:
    df[c] = pd.to_numeric(df[c], errors='coerce')

# FaultFlag present? convert to int (0/1)
if 'FaultFlag' in df.columns:
    df['FaultFlag'] = pd.to_numeric(df['FaultFlag'], errors='coerce').fillna(0).astype(int)

print(df.dtypes)
print("shape:", df.shape)

# parse dates, ensure numeric columns are numeric
date_cols = ['LastServiceDate']  # adjust if different
for c in date_cols:
    df[c] = pd.to_datetime(df[c], dayfirst=True, errors='coerce')  # handle formats like DD-MM-YYYY

# convert sensor numeric-ish columns
numeric_cols = ['Temperature','Pressure','Vibration','Humidity','FlowRate','PowerConsumption','RuntimeHours','SensorID']
for c in numeric_cols:
    df[c] = pd.to_numeric(df[c], errors='coerce')

# FaultFlag present? convert to int (0/1)
if 'FaultFlag' in df.columns:
    df['FaultFlag'] = pd.to_numeric(df['FaultFlag'], errors='coerce').fillna(0).astype(int)

print(df.dtypes)
print("shape:", df.shape)

#missing values handling & basic imputation
# summary
print(df.isna().sum())

# Strategy:
# - For numeric sensors: median imputation (robust to outliers)
# - For date: if missing, leave NaT but create days_since_service later
from sklearn.impute import SimpleImputer
imp_median = SimpleImputer(strategy='median')
df[numeric_cols] = pd.DataFrame(imp_median.fit_transform(df[numeric_cols]), columns=numeric_cols)

# create engineered features
# days since last service
df['DaysSinceService'] = (pd.Timestamp.today().normalize() - df['LastServiceDate']).dt.days
df['DaysSinceService'] = df['DaysSinceService'].fillna(df['DaysSinceService'].median())

# if RuntimeHours extreme? cap by percentile (optional)
cap_high = df['RuntimeHours'].quantile(0.99)
df['RuntimeHours'] = np.where(df['RuntimeHours'] > cap_high, cap_high, df['RuntimeHours'])

#optional smoothing / denoising (simple)
# If the sensors are time-series with timestamps in order, you can apply rolling median to remove spikes.
# If data is snapshot-style (many sensors at one time), skip heavy smoothing.
# Example (only when time-ordered):
if 'Timestamp' in df.columns or True:
    # this example assumes rows are time-ordered per sensor; adjust for real timeseries
    df = df.sort_values(by=['SensorID', 'LastServiceDate']).reset_index(drop=True)
    for c in ['Temperature','Pressure','Vibration','Humidity','FlowRate','PowerConsumption']:
        df[c + '_med'] = df.groupby('SensorID')[c].transform(lambda x: x.rolling(window=3, min_periods=1).median())
    # choose med columns for modeling if you want

#  feature selection for model
# We're using sensor numeric features + DaysSinceService + RuntimeHours
feat_cols = ['Temperature','Pressure','Vibration','Humidity','FlowRate','PowerConsumption','RuntimeHours','DaysSinceService']
X = df[feat_cols].copy()
y = df['FaultFlag'].copy() if 'FaultFlag' in df.columns else None

# scale
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

#  Isolation Forest training
# unsupervised training, but we'll evaluate using y

# Note: if sklearn throws an error about 'behaviour', drop that arg. Using simple call:
iso = IsolationForest(n_estimators=200, contamination=0.05, random_state=RANDOM_STATE)
iso.fit(X_scaled)

# anomaly scores: -1 for anomaly in predict; decision_function gives anomaly score
pred = iso.predict(X_scaled)       # 1 (normal) or -1 (anomaly)
df['anomaly_iforest'] = np.where(pred == -1, 1, 0)
df['anomaly_score'] = -iso.decision_function(X_scaled)  # higher => more anomalous

#  — evaluation vs FaultFlag (if labels exist)
if y is not None:
    y_true = y.values
    y_pred = df['anomaly_iforest'].values
    print("Precision:", precision_score(y_true, y_pred, zero_division=0))
    print("Recall   :", recall_score(y_true, y_pred, zero_division=0))
    print("F1       :", f1_score(y_true, y_pred, zero_division=0))
    try:
        print("ROC AUC  :", roc_auc_score(y_true, df['anomaly_score']))
    except Exception as e:
        print("ROC AUC could not be computed:", e)
    print("\nClassification report:\n", classification_report(y_true, y_pred, zero_division=0))
else:
    print("No ground-truth FaultFlag available — unsupervised scoring only.")

# time-series visualization of one sensor (or aggregated)
# If you have a timestamp column per reading, substitute 'index' with timestamp
plt.figure(figsize=(14,4))
plt.plot(df.index, df['Temperature'], label='Temperature')
# highlight anomalies
anoms = df[df['anomaly_iforest'] == 1]
plt.scatter(anoms.index, anoms['Temperature'], marker='x', s=60, label='Anomaly', zorder=5)
plt.xlabel('record index (or timestamp)')
plt.ylabel('Temperature')
plt.legend()
plt.title('Temperature time-series with anomalies')
plt.show()

# Cell 10 — scatter plot pressure vs vibration to show cluster/outliers
plt.figure(figsize=(7,6))
plt.scatter(df['Pressure'], df['Vibration'], alpha=0.6, s=20)
an = df[df['anomaly_iforest'] == 1]
plt.scatter(an['Pressure'], an['Vibration'], marker='x', s=60)
plt.xlabel('Pressure')
plt.ylabel('Vibration')
plt.title('Pressure vs Vibration (anomalies marked)')
plt.show()

#  inspect top anomalous rows
top_anom = df.sort_values('anomaly_score', ascending=False).head(30)
top_anom[['SensorID','Temperature','Pressure','Vibration','Humidity','FlowRate','PowerConsumption','RuntimeHours','anomaly_score','FaultFlag']].head(20)

# save results
df.to_csv("sensor_results_with_anomaly_flag.csv", index=False)
print("Saved results to sensor_results_with_anomaly_flag.csv")
from sklearn.pipeline import Pipeline
import joblib

pipeline = Pipeline([
    ("scaler", StandardScaler()),
    ("isolation_forest", IsolationForest(n_estimators=200, contamination=0.05, random_state=42))
])

# train pipeline on raw features
pipeline.fit(X)

# save pipeline
joblib.dump(pipeline, "anomaly_detection_pipeline.pkl")

print("Pipeline saved as anomaly_detection_pipeline.pkl")


